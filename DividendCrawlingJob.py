#!/usr/bin/env python
# -*- coding:utf-8 -*-

from __future__ import print_function
import json
import re
import urllib2

"""
    DividendCrawlingJob:
        NOTE: This script was used to make web reqeust to Goodinfo!台灣股市資訊網 and parse the web content for dividend data.
        As the crawling approache envolves, the main crawling logic has been spinned off to fetch_dividend_data.py and this
        script only reads the parsed data from a local file generated by it. The whole task is actually a synchornized task
        but for the history reason the logic here still *looks like** asynchornized.
"""

class DividendCrawlingJob:

    def __init__ ( self, year ):
        data_filename = 'dividend_data/year_' + str(year) + '.json'
        try:
            with open(data_filename, 'r') as rf:
                self.web_req_success = False
                self.parse_success = False
                self.myid = str(type(self)) + '_year_' + str(year)
                self.year = year
                self.data = json.load(rf)
        except:
            pass

    # Note: Called by dedicated thread of WorkPool
    def __call__ ( self ):

        if self.data == None:
            print("DividendCrawlingJob: Found no dividend data at local (year=" + str(year) + "). Try run fetch_dividend_data.py first.")
            return
        
        try:
            collectedData = {}
            for e in self.data.values():
                ###
                # Each e (entry) should be an array that contains:
                #   index  0 : "上市"
                #          1 : Stock ID (should match with key)
                #          2 : Stock Name
                #          3 : Dividend Issue Year
                #          4 : Dividend Belong Year
                #          5 : Annual Profit (Unit: 100M NTD)
                #          6 : EPS
                #      7,8,9 : Cash Dividend (Profit, Capital, Sum)
                #   10,11,12 : Stock Dividend (Profit, Capital, Sum)
                #         13 : Total Dividend

                # Sanity check
                if e[3] != str(self.year):
                    raise RuntimeError('Sanity check failed - year does not match')

                # Parse profit whenever possible
                try:
                    profit = float(e[5].replace(',',''))
                except:
                    profit = None

                # Parse eps whenever possible
                try:
                    eps = float(e[6].replace(',',''))
                except:
                    eps = None

                collectedData[e[1]] = { 
                    'id': e[1], 
                    'name': e[2], 
                    'year': int(e[3]), 
                    'profit': profit,
                    'eps' : eps,
                    'cash_dividend': float(e[9].replace(',','')), 
                    'stock_dividend': float(e[12].replace(',','')) 
                }

            self.data = collectedData
            self.web_req_success = True
            self.parse_success = True

        except Exception as e:
            self.data = None
            print('DividendCrawlingJob: Unexpected exception occurred when parsing devidend data. Exception: ' + str(e))

if __name__ == '__main__':
    # Entry point to quickly verify parsing result
    job = DividendCrawlingJob(2024)
    job()
    print(str(job.data['2330']))

